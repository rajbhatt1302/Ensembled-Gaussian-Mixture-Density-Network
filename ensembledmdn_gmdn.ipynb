{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtZjLbecmVeR",
        "outputId": "c9101b02-1ff9-4fb6-ec24-1d94018b49be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputed data saved to /content/GLORIA_imputed.csv\n"
          ]
        }
      ],
      "source": [
        "#data imputation to replace NaN values\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/GLORIA.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Select columns 2 to the last column (1-indexed)\n",
        "cols_to_impute = data.columns[1:]  # Adjust indexing to 0-indexed Python logic\n",
        "\n",
        "# Apply KNN Imputation\n",
        "imputer = KNNImputer(n_neighbors=5)  # Default n_neighbors=5\n",
        "data[cols_to_impute] = imputer.fit_transform(data[cols_to_impute])\n",
        "\n",
        "# Save the imputed dataset\n",
        "output_path = \"/content/GLORIA_imputed.csv\"\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Imputed data saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ-LHHR6fHr-",
        "outputId": "d83bb21a-80e3-4bb4-8788-c90cd13619e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ensembled mdn (gmdn)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import RandomSearch, HyperParameters\n",
        "\n",
        "# Module 1: Data Preprocessing\n",
        "def preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Preprocess the data by loading it, removing NaN values, and splitting into train and test sets.\n",
        "    Standardize the features and targets using StandardScaler.\n",
        "    \"\"\"\n",
        "    # Load the data\n",
        "    data = pd.read_csv(file_path)\n",
        "    data = data.dropna()  # Remove rows with NaN values\n",
        "\n",
        "    # Extract FID, features, and targets\n",
        "    FID = data.iloc[:, 0].values\n",
        "    features = data.iloc[:, 1:-3].values\n",
        "    targets = data.iloc[:, -3:].values #the target variables are log converted, so apply exponent on the final results to convert to linear\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test, fid_train, fid_test = train_test_split(\n",
        "        features, targets, FID, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Standardize the features and targets\n",
        "    feature_scaler = StandardScaler()\n",
        "    target_scaler = StandardScaler()\n",
        "\n",
        "    X_train = feature_scaler.fit_transform(X_train)\n",
        "    X_test = feature_scaler.transform(X_test)\n",
        "\n",
        "    y_train = target_scaler.fit_transform(y_train)\n",
        "    y_test = target_scaler.transform(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, fid_test, target_scaler\n",
        "\n",
        "# Module 2: Model Builder for Keras Tuner\n",
        "def build_complex_model(hp):\n",
        "    \"\"\"\n",
        "    Build a complex neural network with tunable hyperparameters.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_layer1', min_value=64, max_value=256, step=32),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=l2(hp.Choice('l2_reg', [0.001, 0.01, 0.1])),\n",
        "        input_dim=X_train.shape[1]\n",
        "    ))\n",
        "\n",
        "    # Hidden layers\n",
        "    for i in range(hp.Int('num_hidden_layers', 7, 9)):  # Between 7 and 9 layers\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f'units_layer{i+2}', min_value=32, max_value=128, step=16),\n",
        "            activation='relu',\n",
        "            kernel_regularizer=l2(hp.Choice('l2_reg', [0.001, 0.01, 0.1]))\n",
        "        ))\n",
        "        model.add(Dropout(hp.Float('dropout_rate', 0.1, 0.5, step=0.1)))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(3, activation='linear'))  # 3 target variables\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mae', 'mape']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Module 3: Hyperparameter Tuning\n",
        "def tune_hyperparameters(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Tune hyperparameters using Keras Tuner.\n",
        "    \"\"\"\n",
        "    tuner = RandomSearch(\n",
        "        build_complex_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=10,  # Number of hyperparameter combinations\n",
        "        executions_per_trial=1,\n",
        "        directory='hyperparameter_tuning',\n",
        "        project_name='complex_model_tuning'\n",
        "    )\n",
        "\n",
        "    tuner.search(X_train, y_train, epochs=200, validation_split=0.2, batch_size=100)\n",
        "    return tuner\n",
        "\n",
        "# Module 4: Training the Best Models for Ensemble\n",
        "def train_ensemble_models(tuner, X_train, y_train, num_ensemble=10):\n",
        "    \"\"\"\n",
        "    Train an ensemble of models (10 models) and return the best 3 based on validation loss.\n",
        "    \"\"\"\n",
        "    best_models = []\n",
        "    histories = []\n",
        "    best_hps = []\n",
        "\n",
        "    # Train 10 models and track their performance\n",
        "    for i in range(num_ensemble):\n",
        "        best_hps_i = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        model = tuner.hypermodel.build(best_hps_i)\n",
        "\n",
        "        # Train the model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "        history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=100, callbacks=[early_stopping])\n",
        "\n",
        "        best_models.append(model)\n",
        "        histories.append(history)\n",
        "        best_hps.append(best_hps_i)\n",
        "\n",
        "    # Select top 3 models based on validation loss\n",
        "    val_losses = [min(history.history['val_loss']) for history in histories]\n",
        "    best_indices = np.argsort(val_losses)[:3]\n",
        "\n",
        "    top_3_models = [best_models[i] for i in best_indices]\n",
        "    return top_3_models, histories, best_hps, val_losses\n",
        "\n",
        "# Module 5: Averaging Predictions from Ensemble\n",
        "def average_predictions(top_3_models, X_test, target_scaler):\n",
        "    \"\"\"\n",
        "    Average the predictions from the top 3 models.\n",
        "    \"\"\"\n",
        "    predictions_list = []\n",
        "    for model in top_3_models:\n",
        "        predictions = model.predict(X_test)\n",
        "        predictions_denormalized = target_scaler.inverse_transform(predictions)\n",
        "        predictions_list.append(predictions_denormalized)\n",
        "\n",
        "    # Average the predictions of the top 3 models\n",
        "    averaged_predictions = np.mean(predictions_list, axis=0)\n",
        "    return averaged_predictions\n",
        "\n",
        "# Module 6: Saving Results and Plotting\n",
        "def save_results_and_plot(top_3_models, histories, averaged_predictions, X_test, y_test, fid_test, target_scaler, output_dir='results'):\n",
        "    \"\"\"\n",
        "    Save the trained models, test results, loss plots, and averaged predictions.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Save the models\n",
        "    for i, model in enumerate(top_3_models):\n",
        "        model.save(f'{output_dir}/best_model_{i+1}.h5')\n",
        "\n",
        "    # Predict and denormalize\n",
        "    y_test_denormalized = target_scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Save test results\n",
        "    results = pd.DataFrame(fid_test, columns=['FID'])\n",
        "    results = pd.concat([results, pd.DataFrame(y_test_denormalized, columns=['True1', 'True2', 'True3'])], axis=1)\n",
        "    results = pd.concat([results, pd.DataFrame(averaged_predictions, columns=['Pred1', 'Pred2', 'Pred3'])], axis=1)\n",
        "    results.to_csv(f'{output_dir}/ensemble_test_results.csv', index=False)\n",
        "\n",
        "    # Save loss data for each model\n",
        "    for i, history in enumerate(histories):\n",
        "        loss_data = {\n",
        "            'Epochs': list(range(1, len(history.history['loss']) + 1)),\n",
        "            'Training Loss': history.history['loss'],\n",
        "            'Validation Loss': history.history['val_loss']\n",
        "        }\n",
        "        loss_df = pd.DataFrame(loss_data)\n",
        "        loss_df.to_csv(f'{output_dir}/loss_data_model_{i+1}.csv', index=False)\n",
        "\n",
        "    # Plot and save the loss curves for each model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.plot(history.history['loss'], label=f'Model {i+1} Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label=f'Model {i+1} Validation Loss')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss for Each Model')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(f'{output_dir}/ensemble_loss_plot.png')\n",
        "    plt.show()\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = 'GLORIA_imputed.csv'  # Adjust file path if necessary\n",
        "    X_train, X_test, y_train, y_test, fid_test, target_scaler = preprocess_data(file_path)\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    tuner = tune_hyperparameters(X_train, y_train)\n",
        "\n",
        "    # Train the ensemble of models\n",
        "    top_3_models, histories, best_hps, val_losses = train_ensemble_models(tuner, X_train, y_train)\n",
        "\n",
        "    # Average the predictions of the top 3 models\n",
        "    averaged_predictions = average_predictions(top_3_models, X_test, target_scaler)\n",
        "\n",
        "    # Save results and plot\n",
        "    save_results_and_plot(top_3_models, histories, averaged_predictions, X_test, y_test, fid_test, target_scaler)\n",
        "\n",
        "    # Print best hyperparameters for each model\n",
        "    for i, best_hp in enumerate(best_hps):\n",
        "        print(f\"\"\"\n",
        "        Best Hyperparameters for Model {i+1}:\n",
        "        - Units Layer 1: {best_hp.get('units_layer1')}\n",
        "        - L2 Regularization: {best_hp.get('l2_reg')}\n",
        "        - Dropout Rate: {best_hp.get('dropout_rate')}\n",
        "        - Learning Rate: {best_hp.get('learning_rate')}\n",
        "        - Number of Hidden Layers: {best_hp.get('num_hidden_layers')}\n",
        "        \"\"\")\n"
      ],
      "metadata": {
        "id": "BdihDYwHe8it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the paths to the directories and file\n",
        "paths_to_zip = [\n",
        "    '/content/hyperparameter_tuning',\n",
        "    '/content/results',\n",
        "    '/content/GLORIA_imputed.csv'\n",
        "]\n",
        "\n",
        "# Define the name of the zip file\n",
        "zip_filename = '/content/combined_files.zip'\n",
        "\n",
        "# Create a Zip file\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for path in paths_to_zip:\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(path):\n",
        "            # Walk through the directory and add all files\n",
        "            for foldername, subfolders, filenames in os.walk(path):\n",
        "                for filename in filenames:\n",
        "                    file_path = os.path.join(foldername, filename)\n",
        "                    zipf.write(file_path, os.path.relpath(file_path, '/content'))\n",
        "        else:\n",
        "            # Add the file to the zip\n",
        "            zipf.write(path, os.path.basename(path))\n",
        "\n",
        "print(f\"Zip file created: {zip_filename}\")\n"
      ],
      "metadata": {
        "id": "92wPqZzCfL7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc0ad18-114f-42c3-98ac-4bd701bafad1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file created: /content/combined_files.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finetune the saved ensembled models for new data\n",
        " from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_best_models(model_dir):\n",
        "    \"\"\"\n",
        "    Load the top 3 models from the specified directory.\n",
        "    \"\"\"\n",
        "    models = []\n",
        "    for i in range(1, 4):  # Assuming models are named best_model_1.h5, best_model_2.h5, best_model_3.h5\n",
        "        model_path = os.path.join(model_dir, f'best_model_{i}.h5')\n",
        "        models.append(load_model(model_path))\n",
        "    return models\n",
        "\n",
        "def fine_tune_model(model, X_train, y_train, learning_rate=1e-4, fine_tune_epochs=50):\n",
        "    \"\"\"\n",
        "    Fine-tune the loaded model with a new optimizer and learning rate.\n",
        "    \"\"\"\n",
        "    # Re-compile the model with a new optimizer\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mae', 'mape']\n",
        "    )\n",
        "\n",
        "    # Perform fine-tuning\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=fine_tune_epochs,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def average_predictions_from_saved_models(models, X_test, target_scaler):\n",
        "    \"\"\"\n",
        "    Compute the average predictions from the loaded models.\n",
        "    \"\"\"\n",
        "    predictions_list = []\n",
        "    for model in models:\n",
        "        predictions = model.predict(X_test)\n",
        "        predictions_denormalized = target_scaler.inverse_transform(predictions)\n",
        "        predictions_list.append(predictions_denormalized)\n",
        "\n",
        "    averaged_predictions = np.mean(predictions_list, axis=0)\n",
        "    return averaged_predictions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the test data\n",
        "    file_path = '/content/data_target.csv'\n",
        "    X_train, X_test, y_train, y_test, fid_test, target_scaler = preprocess_data(file_path)\n",
        "\n",
        "    # Load the top 3 models\n",
        "    model_dir = '/content/results'\n",
        "    models = load_best_models(model_dir)\n",
        "\n",
        "    # Fine-tune the models\n",
        "    fine_tuned_models = []\n",
        "    for i, model in enumerate(models):\n",
        "        print(f\"Fine-tuning Model {i+1}...\")\n",
        "        fine_tuned_model = fine_tune_model(model, X_train, y_train)\n",
        "        fine_tuned_models.append(fine_tuned_model)\n",
        "\n",
        "    # Save fine-tuned models and their weights\n",
        "    output_dir = '/content/finetuned'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "for i, fine_tuned_model in enumerate(fine_tuned_models):\n",
        "    # Save full model\n",
        "    fine_tuned_model.save(f'{output_dir}/fine_tuned_model_{i+1}.h5')\n",
        "\n",
        "    # Save only weights with the correct extension\n",
        "    fine_tuned_model.save_weights(f'{output_dir}/fine_tuned_model_{i+1}.weights.h5')\n",
        "\n",
        "\n",
        "    # Generate averaged predictions\n",
        "    averaged_predictions = average_predictions_from_saved_models(fine_tuned_models, X_test, target_scaler)\n",
        "\n",
        "    # Denormalize ground truth\n",
        "    y_test_denormalized = target_scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Save results to a CSV\n",
        "    results = pd.DataFrame(fid_test, columns=['FID'])\n",
        "    results = pd.concat([results, pd.DataFrame(y_test_denormalized, columns=['True1', 'True2', 'True3'])], axis=1)\n",
        "    results = pd.concat([results, pd.DataFrame(averaged_predictions, columns=['Pred1', 'Pred2', 'Pred3'])], axis=1)\n",
        "    results.to_csv(f'{output_dir}/final_predictions.csv', index=False)\n",
        "\n",
        "    # Plot true vs predicted values\n",
        "    for i in range(3):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(y_test_denormalized[:, i], averaged_predictions[:, i], alpha=0.6)\n",
        "        plt.plot([y_test_denormalized[:, i].min(), y_test_denormalized[:, i].max()],\n",
        "                 [y_test_denormalized[:, i].min(), y_test_denormalized[:, i].max()], 'r--')\n",
        "        plt.xlabel(f'True Value {i+1}')\n",
        "        plt.ylabel(f'Predicted Value {i+1}')\n",
        "        plt.title(f'True vs Predicted for Target {i+1}')\n",
        "        plt.grid()\n",
        "        plt.savefig(f'{output_dir}/true_vs_pred_target_{i+1}.png')\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"Fine-tuned models, weights, and results saved to {output_dir}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uuycxmRgoXLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zip finetuned folder for download\n",
        "import shutil\n",
        "\n",
        "# Define the folder path\n",
        "folder_path = '/content/finetuned'\n",
        "\n",
        "# Define the output zip file path\n",
        "zip_file_path = '/content/finetuned.zip'\n",
        "\n",
        "# Create a zip file of the folder\n",
        "shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n",
        "\n",
        "print(f\"Folder {folder_path} has been zipped to {zip_file_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLN47jARxXQS",
        "outputId": "4ee0f8cb-56bd-49c9-bcbf-5fedf136e8c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder /content/finetuned has been zipped to /content/finetuned.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the finetuned models on unseen data with only predictor variables\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Step 1: Load the new dataset\n",
        "new_data = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Assuming 'FID' is in the first column and the remaining are features\n",
        "FID_new = new_data.iloc[:, 0].values\n",
        "X_new = new_data.iloc[:, 1:].values\n",
        "\n",
        "# Step 2: Preprocess the data (Standardize the features using the same scaler used during training)\n",
        "# Use the feature scaler from preprocessing\n",
        "feature_scaler = StandardScaler()\n",
        "X_new_scaled = feature_scaler.fit_transform(X_new)\n",
        "\n",
        "# Step 3: Load the top 3 best models saved in /content/results/\n",
        "top_3_models = []\n",
        "for i in range(3):\n",
        "    model = load_model(f'/content/finetuned/fine_tuned_model_{i+1}.h5')\n",
        "    top_3_models.append(model)\n",
        "\n",
        "# Step 4: Use the top 3 models to generate predictions\n",
        "predictions_list = []\n",
        "for model in top_3_models:\n",
        "    predictions = model.predict(X_new_scaled)\n",
        "    predictions_list.append(predictions)\n",
        "\n",
        "# Step 5: Average the predictions from the 3 models\n",
        "averaged_predictions = np.mean(predictions_list, axis=0)\n",
        "\n",
        "# Step 6: Denormalize the predictions using the target scaler\n",
        "# Assuming `target_scaler` is the scaler used during training for target variables\n",
        "averaged_predictions_denormalized = target_scaler.inverse_transform(averaged_predictions)\n",
        "\n",
        "# Step 7: Combine FID and predictions into a final result DataFrame\n",
        "results = pd.DataFrame(FID_new, columns=['FID'])\n",
        "results[['Pred1', 'Pred2', 'Pred3']] = averaged_predictions_denormalized\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results.to_csv('/content/ensemble_predictions.csv', index=False)\n",
        "\n",
        "# Display the results\n",
        "print(results.head())\n"
      ],
      "metadata": {
        "id": "slNgmLECtU5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}